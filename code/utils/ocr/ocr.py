"""
Optical Character Recognizer Class
Mike Hilton, Eckerd College
2021/07/06

This module implements the class TrailCamOCR, which is an image-to-text converter 
for extracting the date, time, and camera serial number fields embedded in an information
banner burned into the trail camera images.  To extract the values of the text fields,
create an instance of the TrailCamOCR class and call the extractImageInfo(imageFilename)
method.

This implementation was designed to work with a particular camera model, the Meidase SL122 Pro,
which burns an information banner into the bottom of each image. The banner is a gray translucent
ribbon with white text.

A dictionary of "codebooks" is used to allow the program to work with the various sizes
of image generated by the camera.  Each image size has its own codebook describing the
expected location of each digit, the function to use for decoding this image, and the
name of the data file associated with the decoding function.

There are two example OCR models included in this code distribution, a k-Nearest Neighbor
classifier and a Convolutional Neural Net classifier.

The k-Nearest Neighbor algorithm used here works roughly as follows: 
1. Extract the image blocks where we expect the text to be and perform some image
    processing operations on the blocks to binarize the block into black and white
    pixels.
2. Count the number of white pixels in the uppermost 60% of the block and also the
    leftmost 60% of the block, yeilding two numbers.
    The reason for counting two portions of the block is that several of the digits 
    have similar numbers of white pixels, so just counting all the white pixels in 
    the block is not good enough to discriminate between digits reliably.  Experiments 
    were run comparing different counting schemes, and the one described above performed
    the best.
3. Use the k-Nearest Neighbors (k-NN) algorithm to determine the digit. k-NN is a
    machine learning algorithm that compares the pair of pixel counts to a database of 
    known values.
If you want to create a k-NN database for your camera, you should start by creating a
new codebook and getting the box coordinates set up.  Then you can use the
generateTrainingData() method to create a data file containing pixel counts.
I load this file into RStudio to graphically examine the data and to test how well k-NN 
works with the data.  Eventually, you should create a CSV data file containing three
columns -- digit, top, and left -- in that order.

The Convolutional Neural Network uses an architecture found in many tutorials on digit
recognition that use the MNIST handwritten digits dataset.  If you want to create a CNN
classifier for your camera, see the file "ocr_digit_convnet_training.py" for an example.

"""

# standard Python modules
from datetime import datetime
import os
import random

# 3rd party modules
import cv2
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
import tensorflow as tf


# modules that are part of this library
from .. trailcamutils import createDatetime, parseSerialNumber, getFilePathsInSubfolders, splitImageFilename


class TrailCamOCR:
    DEBUG_OCR = False   # setting this to True causes information about pixel counts for each block to be printed

    def __init__(self):
        # self.codebooks is organized by camera model and image size.  Each image size dictionary should contain
        # the following keys:
        #   function        digit extraction function for this image
        #   datafile        path to file containing the model for this image size
        #   xwidth          width of image block containing digit
        #   ycoords         the starting and ending y-coordinates for all text blocks, as a pair
        #   date            the starting x-coordinates of each digit in a YYYY/MM/DD date
        #   serialNumber    the starting x-coordinates of each digit in the four-digit camera serial number
        #   time            the starting x-coordinates of each digit in a HH:MM:SS time        
        self.codebooks = { 
            # Meidase SL122 Pro camera, 4MP images              
            "meidase_SL122_Pro_2560x1440" : {
                ## k-NN model
                # "function": self.digit_meidase_kNN,
                # "datafile": "digits_kNN_meidase_2560x1440.csv",
                ## CONVNET model
                "function": self.digit_meidase_CNN,
                "datafile": "digits_CNN_meidase_2560x1440.keras", 

                "xwidth" : 46,
                "ycoords" : (1334,1399),
                "date" : [992, 1040, 1088, 1135, 1216, 1264, 1345, 1394],
                "serialNumber" : [2176, 2224, 2272, 2320],
                "time" : [1462, 1510, 1580, 1627, 1698, 1746] 
            }                  
        }


    def create_CNN(self, codebook):
        """
        Creates a Convolutional Neural Network classifier and stores it in
        the codebook.
        Inputs:
            codebook        codebook dictionary for this image type          
        """
        model_file = os.path.join(os.path.split(__file__)[0], codebook["datafile"])
        codebook["cnn"] = tf.keras.models.load_model(model_file)       


    def create_kNN(self, codebook):
        """
        Creates and initializes a k-Nearest Neighbors classifier, and stores
        it in the codebook.
        Inputs:
            codebook        codebook dictionary for this image type       
        """
        pairs = []
        digits = []

        # read in the data file associated with this codebook
        # file columns are in the order: digit, top count, left count
        datafile = os.path.join(os.path.split(__file__)[0], codebook["datafile"])
        with open(datafile, "r") as fp:
            # ignore the column headers
            fp.readline()
            # read in the data pairs
            for aLine in fp:
                parts = aLine.split(",")
                if len(parts) == 3:
                    digits.append(parts[0].strip())
                    pairs.append([int(parts[1].strip()), int(parts[2].strip())])

        # initialize the classifier
        classifier = KNeighborsClassifier(n_neighbors=3)
        classifier.fit(pairs, digits)
        codebook["kNN"] = classifier


    def digit_meidase_CNN(self, codebook, field, index, img):
        """
        OCR function for Meidase SL122 Pro camera images, featuring white digits on a 
        translucent background. 
        Determines which digit is in the specified box.
        Inputs:
            codebook        codebook dictionary for this image type
            field           string; field to extract
            index           integer; index of digit in field to extract
            img             grayscale numpy image
        Returns:
            The digit in the box, as a string.
        """ 
        # check if a classifier has been initialized for this codebook
        if "cnn" not in codebook:
            self.create_CNN(codebook)

        # pixels with value >= threshold are considered white
        threshold = 245

        # get block locations
        yStart, yEnd = codebook["ycoords"]
        blocks = codebook[field]
        width = codebook["xwidth"]

        # extract the digit bounding box            
        x = blocks[index]
        box = np.array(img[yStart:yEnd, x:(x+width)])        

        # threshold the box for white pixels
        e = np.greater(box, threshold).astype(np.uint8) * 255
        # close up the small holes
        kernel = np.ones((3,3),np.uint8)
        pixels = cv2.morphologyEx(e, cv2.MORPH_CLOSE, kernel)

        # convert the pixels to floats
        pixels = pixels.astype("float32") / 255
        pixels = np.asarray([np.expand_dims(pixels, -1)])

        # apply the model
        pred = np.argmax(codebook["cnn"](pixels), axis=1)
        return str(pred[0])
    

    def digit_meidase_kNN(self, codebook, field, index, img):
        """
        OCR function for Meidase SL122 Pro camera images, featuring white digits on a 
        translucent background. 
        Determines which digit is in the specified box.
        Inputs:
            codebook        codebook dictionary for this image type
            field           string; field to extract
            index           integer; index of digit in field to extract
            img             grayscale numpy image
        Returns:
            The digit in the box, as a string.
        """ 
        # check if a classifier has been initialized for this codebook
        if "kNN" not in codebook:
            self.create_kNN(codebook)

        # pixels with value >= threshold are considered white
        threshold = 250 #245

        # get block locations
        yStart, yEnd = codebook["ycoords"]
        blocks = codebook[field]
        width = codebook["xwidth"]

        # extract the digit bounding box            
        x = blocks[index]
        box = np.array(img[yStart:yEnd, x:(x+width)])        

        # threshold the box for white pixels
        e = np.greater(box, threshold).astype(np.uint8) * 255
        # close up the small holes
        kernel = np.ones((3,3),np.uint8)
        pixels = cv2.morphologyEx(e, cv2.MORPH_CLOSE, kernel)

        # sum up the white pixels in 60% box
        sTop = int(np.sum(pixels[0:int(pixels.shape[0] * 0.6),]) // 255)
        sLeft = int(np.sum(pixels[0:pixels.shape[0], 0:int(pixels.shape[1] * 0.6)]) // 255)

        if TrailCamOCR.DEBUG_OCR:
            # debugging statements for use when creating digit counts        
            cv2.imwrite(f"{sTop}-{sLeft}-bw.jpg", pixels)

        # look up the digit
        return codebook["kNN"].predict([[sTop, sLeft]])[0]


    def extractDate(self, img, cameraModel, sep=""):
        digits = self.extractDigits(img, "date", cameraModel)
        parts = [ "".join(digits[:4]), "".join(digits[4:6]), "".join(digits[6:])]
        return sep.join(parts)


    def extractDigits(self, img, field, cameraModel):
        """
        Extract a sequence of digits from the info strip at the bottom.
        Inputs:
            img             2D numpy array containing image being analyzed
            field           string; name of the field to extract - one of "date", "serialNumber", "temperature", "time"
            cameraModel     string; unique identifier for the camera model used to take image
        Returns:
            A string containing the extracted digits
        """
        # get the codebook for images of this size
        imgSize = f"{cameraModel}_{img.shape[1]}x{img.shape[0]}"
        if imgSize not in self.codebooks:
            raise KeyError(f"No codebook for {cameraModel} images of size {imgSize}")
        codebook = self.codebooks[imgSize]

        blocks = codebook[field]

        # extract the digits
        answer = []
        for i in range(len(blocks)):
            # call the appropriate OCR function for this codebook
            answer.append(codebook["function"](codebook, field, i, img))                      

        return answer


    def extractImageInfo(self, filename, views={}, cameraModel="meidase_SL122_Pro"):
        """
        Extract information from the text burned into the bottom of a trail camera image.
        The cameraModel and image size are combined to determine the name of the codebook
        Inputs:
            filename        string; path to image file
            views           dictionary mapping digits to (full view name, abbreviated view name)
            cameraModel     string; unique identifier for the camera model used to take image
        Returns:
            A tuple of the form (camera_ID, abbreviated_view, date, time)
        """
        # load a grayscale version of the image
        grayImg = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)    
        if grayImg is None:
            raise Exception("extractImageInfo: Unable to read image " + filename)     
        
        # extract text from image using OCR
        date = self.extractDate(grayImg, cameraModel, "-")
        time = self.extractTime(grayImg, cameraModel)
        serialNumber = self.extractSerialNumber(grayImg, cameraModel)

        # If the date or time is not valid, the image is probably corrupt.
        # The following code will throw an error if the date or time is not valid.
        dt = datetime.strptime(createDatetime(date, time), "%Y%m%d-%H%M%S")

        # if the date is in the future, the image is corrupt
        if (dt > datetime.now()):
            raise Exception("Trailcam OCR date is in the future; image is probably corrupt")

        # parse the serial number
        camera_ID, view, _ = parseSerialNumber(serialNumber, views)
        return camera_ID, view, date, time


    def extractTime(self, img, cameraModel, sep=""):
        """
        Return the time field from an image.
        """
        digits = self.extractDigits(img, "time", cameraModel)
        parts = [ "".join(digits[:2]), "".join(digits[2:4]), "".join(digits[4:])]
        return sep.join(parts)


    def extractSerialNumber(self, img, cameraModel):
        """
        Returns the camera serial number field from an image.
        """
        digits = self.extractDigits(img, "serialNumber", cameraModel)
        return "".join(digits)


    def fieldPixelImages(self, img, field, label, sampleCounts, maxSamples, outputFolder, 
                         cameraModel="meidase_SL122_Pro", threshold=245):
        """
        This routine is used for debugging and designing new OCR functions.  It extracts the
        pixels in the digits for a specified field and writes them to the outputFolder.
        Inputs:
            img             a grayscale numpy image
            field           string; name of the field to extract the digit values for
            label           string; the known values of the extracted digits
            sampleCounts    dictionary mapping digit to count of sample images extracted for digit so far
            maxSamples      int; maximum number of sample images to extract per digit
            outputFolder    string; path to master folder where digit images will be written
            cameraModel     string; unique identifier for the camera model used to take image    
            threshold       int; pixels with values >= threshold are considered to be white     
        """
        # get the codebook for images of this size
        imgSize = f"{cameraModel}_{img.shape[1]}x{img.shape[0]}"
        if imgSize not in self.codebooks:
            raise KeyError("No codebook for images of size " + imgSize)
        codebook = self.codebooks[imgSize]

        # get block locations
        yStart, yEnd = codebook["ycoords"]
        blocks = codebook[field]
        width = codebook["xwidth"]

        # extract the digits
        for i in range(len(blocks)):
            # test if we already have enough samples of this digit
            if sampleCounts[label[i]] >= maxSamples:
                continue

            # extract the digit bounding box
            x = blocks[i]            
            box = np.array(img[yStart:yEnd, x:(x+width)])
            # threshold the box for white pixels
            e = np.greater(box, threshold).astype(np.uint8) * 255
            # close up the small holes
            kernel = np.ones((3,3),np.uint8)
            pixels = cv2.morphologyEx(e, cv2.MORPH_CLOSE, kernel)
            
            # write the pixels image to output folder
            folder = os.path.join(outputFolder, label[i])
            os.makedirs(folder, exist_ok=True)
            cv2.imwrite(os.path.join(folder, f"{label[i]}_{sampleCounts[label[i]]}.png"), pixels)
            sampleCounts[label[i]] += 1
    
 
    def fieldPixelValues(self, img, field, frac, cameraModel="meidase_SL122_Pro", threshold=245):
        """
        This routine is used for debugging and designing new OCR functions.  It counts the
        number of white pixels in the digits for a specified field.
        Inputs:
            img             a grayscale numpy image
            field           string; name of the field to extract the digit values for
            frac            float; the fraction of the digit box to count white pixels in
            cameraModel     string; unique identifier for the camera model used to take image    
            threshold       int; pixels with values >= threshold are considered to be white     
        Output:
            A list containing triples for each digit box in the field, consisting of:
                (
                    count of all white pixels in the box, 
                    count of all white pixels in the top-most frac of the box,
                    count of all white pixels in the left-most frac of the box
                )
        """
        # get the codebook for images of this size
        imgSize = f"{cameraModel}_{img.shape[1]}x{img.shape[0]}"
        if imgSize not in self.codebooks:
            raise KeyError("No codebook for images of size " + imgSize)
        codebook = self.codebooks[imgSize]

        # get block locations
        yStart, yEnd = codebook["ycoords"]
        blocks = codebook[field]
        width = codebook["xwidth"]

        # extract the digits
        answer = []
        for i in range(len(blocks)):
            x = blocks[i]
       
            # extract the digit bounding box
            box = np.array(img[yStart:yEnd, x:(x+width)])
            # threshold the box for white pixelslook up
            e = np.greater(box, threshold).astype(np.uint8) * 255
            # close up the small holes
            kernel = np.ones((3,3),np.uint8)
            pixels = cv2.morphologyEx(e, cv2.MORPH_CLOSE, kernel)

            # sum up the white pixels
            sAll = int(np.sum(pixels) // 255)
            sTop = int(np.sum(pixels[0:int(pixels.shape[0] * frac),]) // 255)
            sLeft = int(np.sum(pixels[0:pixels.shape[0], 0:int(pixels.shape[1] * frac)]) // 255)
            answer.append((sAll, sTop, sLeft))
        return answer


    def generateTrainingDataKNN(self, folder, frac, outputFilename, maxFiles=10_000, 
            cameraModel="meidase_SL122_Pro", prefix="B", views = {0:("Top","T"), 1:("Frontal", "F")}):
        """
        Generate data for training k-NN classifiers by extracting pixel counts from 
        the image banner fields.

        This method assumes the images have filenames in the format produced by the 
        autocopy program.  I suggest you use autocopy with the configuration setting
        use_exif = 1 and you also supply a camera_id.  This will create a set of images
        that you can then process with this function.        
        """
        def writeData(field, vals, digits):
            nonlocal fp            
            for i in range(len(vals)):
                all, top, left = vals[i]
                fp.write(f"{digits[i]},{field},{i},{all},{top},{left}\n")


        # collect file names
        files = getFilePathsInSubfolders(folder)
        if len(files) > maxFiles:
            files = files[:maxFiles]

        with open(outputFilename, "w") as fp:
            # write column headers
            fp.write("digit,field,index,all,top,left\n")

            for file in files:
                # read image file
                img = cv2.imread(file, cv2.IMREAD_GRAYSCALE) 
                if img is None:
                    raise Exception("generateTrainingDataKNN: Unable to read image " + file)   

                _, camera_ID, _, date, time = splitImageFilename(file, prefix, views)

                vals = self.fieldPixelValues(img, "date", frac, cameraModel)
                writeData("date", vals, date.replace("-", ""))
                vals = self.fieldPixelValues(img, "time", frac, cameraModel)
                writeData("time", vals, time.replace(":", ""))                
                vals = self.fieldPixelValues(img, "serialNumber", frac, cameraModel)
                writeData("serial", vals[1:], camera_ID)


    def generateTrainingDigitImages(self, inputFolder, outputFolder, threshold=245, maxSamples=1000, 
            cameraModel="meidase_SL122_Pro", prefix="B", views = {0:("Top","T"), 1:("Frontal", "F")}):
        """
        Generate data for training digit classifiers by extracting black & white images of digits from 
        the image banner fields.

        This method assumes the images have filenames in the format produced by the 
        autocopy program.  I suggest you use autocopy with the configuration setting
        use_exif = 1 and you also supply a camera_id.  This will create a set of images
        that you can then process with this function.   

        Inputs:
            inputFolder     string; path to folder containing source images, can contain subfolders
            outputFolder    string; path to folder where digit images will be written            
            threshold       int between 0 and 255; pixel values < threshold are black, otherwise white
            maxSamples      int; maximum number of digit images to extract for each digit
            cameraModel     string; unique identifier for the camera model used to take image  
            prefix          string; image name prefix
            views           dictionary containing the camera views
        """

        # collect file names
        files = getFilePathsInSubfolders(inputFolder)
        random.shuffle(files)

        # create a dictionary to keep track of how many samples are extracted for each digit
        sampleCounts = { str(i): 0 for i in range(10) }

        for file in files:
            # check if we are done
            if sum(sampleCounts.values()) >= 10 * maxSamples:
                break

            # read image file
            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE) 
            if img is None:
                raise Exception("generateTrainingDigitImages: Unable to read image " + file)   

            _, camera_ID, _, date, time = splitImageFilename(file, prefix, views)
            date = date.replace("-", "")
            time = time.replace(":", "")
            camera_ID = "0" + camera_ID

            self.fieldPixelImages(img, "date", date, sampleCounts, maxSamples, outputFolder, cameraModel)
            self.fieldPixelImages(img, "time", time, sampleCounts, maxSamples, outputFolder, cameraModel)  
            self.fieldPixelImages(img, "serialNumber", camera_ID, sampleCounts, maxSamples, outputFolder, cameraModel)            


    def testFolderOfImages(self, folder, cameraModel="meidase_SL122_Pro", 
            views={'0':("Top","T"), '1':("Frontal", "F")},  prefix="B", maxFiles=10_000):
        """
        Compares extracted information with metadata from filename.  A message is
        printed out if the data do not match.

        This method assumes the images have filenames in the format produced by the 
        autocopy program.  I suggest you use autocopy with the configuration setting
        use_exif = 1 and you also supply a camera_id.  This will create a set of images
        that you can test with this function.
        """
        files = getFilePathsInSubfolders(folder, ".JPG")
        if len(files) > maxFiles:
            files = files[:maxFiles]

        for file in files:
            try:
                camera_ID, view, date, time = self.extractImageInfo(file, views, cameraModel)
                camera_ID = camera_ID[1:]
                camera2, camera_ID2, view2, date2, time2 = splitImageFilename(file, prefix, views)
                time2 = time2.replace(":", "")
                if camera_ID != camera_ID2:
                    print("camera_ID:", camera_ID, camera_ID2)
                if view != view2:
                    print("view:", view, view2)
                if date != date2:
                    print("date:", date, date2)
                if time != time2:
                    print("time:", time, time2)
            except Exception as e:
                print(file, e)

