"""
Run Animal Detector on Video File
Mike Hilton, Eckerd College
2022/06/02

This program runs the animal detector on an mp4 video file that was created by the
standard Eckerd Camera Trap Tools workflow. The primary use case for this program 
is to test different animal detectors on the same video.  To replace the default
generic animal detector, you need to modify the ProcessVideo.__init__() method;
see the comments in that method.

Command line arguments for this program are:
    -c, --config    <path>      Configuration file. Optional; if not provided, defaults 
								to "process_video.config" located in the same directory 
								as this program.
	-i,	--input		<path>		Video file to process.  Required.

This program reads the following values from the configuration file:
[General_Settings] default_annotation_folder
[Create_Annotations] sequence_break_threshold

The result of running this program is to create:
- A ".annotations" file containing a draft segmentation of the video, located in the 
  	default Annotations folder.
- A ".vboxes" file containing the bounding boxes generated by the animal detector,
	located in the same folder as the input video file.
You should take steps to prevent the existing annotations and vboxes files from being
overwritten by copying them somewhere safe.
"""

# standard Python modules
import argparse
import configparser
import os

# 3rd party modules
import cv2
import numpy as np
from PIL import Image

# modules that are part of this package
import utils.trailcamutils as trailcamutils


class AppConfig:
	""" Configuration settings for this application """
	def __init__(self, config_filename):
		"""
		Loads settings from the app configuration file.  The settings are taken
		from groups belonging to several apps in the Camera Trap Tools suite.
		"""
		self.app_config = configparser.ConfigParser()         
		if os.path.exists(config_filename):
			self.app_config.read(config_filename)

		# settings common to all Camera Trap Tools applications
		settings = self.app_config["General_Settings"]
		self.annotation_folder = settings["default_annotation_folder"]	 # path to folder containing .annotation files
		# settings taken from other applications
		settings = self.app_config["Create_Annotations"]
		self.sequence_break_threshold = int(settings["sequence_break_threshold"])   # number of non-detected images needed to indicate break in sequence


class ProcessVideo:
	def __init__(self, config_file=None):
		# read the configuration file
		if config_file is None:
			config_file = self.getConfigFilename()
		self.app_config = AppConfig(config_file)

		##### if you implement your own animal detector, you should replace these lines
		## the GenericDetector is a classification-based CNN
		from utils.genericdetector.generic_classification_detector import GenericDetector
		self.object_detector = GenericDetector(self.app_config)
		## the TortoiseDetector is an object detection-based CNN
		# from utils.tortoisedetector.tortoise_detector import TortoiseDetector
		# self.object_detector = TortoiseDetector(self.app_config) 		


	def getConfigFilename(self):
		"""
		Creates a config filename from the main module's file name.
		"""
		base, _ = os.path.splitext(__file__)
		return base + ".config"


	def main(self):
		"""
		Main application driver.
		"""
		# read in the index file, if present
		index_filename = os.path.splitext(self.app_config.input_file)[0] + ".index"
		if os.path.exists(index_filename):
			frames = self.readIndexFile(index_filename)
		else:
			raise Exception("No .index file found!")

		# run the model on input video	
		annotations, boxes_list = self.run_inference(frames)	

		# write the annotation file
		annot_filename = os.path.join(
			self.app_config.annotation_folder,
			os.path.splitext(os.path.basename(self.app_config.input_file))[0] + ".annotations_1"
		)		
		self.writeAnnotationFile(annot_filename, annotations)

		# write the video boxes file			
		box_filename = os.path.splitext(self.app_config.input_file)[0] + ".vboxes_1"
		self.writeBoxFile(box_filename, boxes_list)


	def readIndexFile(self, filename):
		"""
		Read the contents of a video index file, which lists the datetime of each frame.
		Input:
			filename		string; path to video index file
		Returns:
			list of strings
		"""
		frames = []
		with open(filename, "r") as inFile:
			for dt in inFile:
				parts = dt.strip().split("-")
				frames.append(trailcamutils.createDatetime(parts[0], parts[1]))
		return frames


	def run_inference(self, frames):
		"""
		Runs the object detector on all the frames of the source video file.
		Inputs:
			frames				list[string]; datetime of each frame in video				
		Returns:
			A tuple of the form (annotation_list, boxes_list), where
				annotation_list is a list of (start_time, end_time) of segments containing animals
				boxes_list is a tuple (index, list of bounding boxes)
		"""
		# open the video source
		video_src = cv2.VideoCapture(self.app_config.input_file)

		# annotation tracking variables
		start_idx = -1
		prev_idx = 0

		annotation_list = []
		boxes_list = []
		index = 0		# frame counter	
		while(video_src.isOpened()):
			ret, frame = video_src.read()
			if not ret:
				break

			# run the object detector	
			image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)				
			boxes = self.object_detector.detectFromImage(image) 

			if len(boxes) > 0:
				boxes_list.append((index, boxes))

				# annotation tracking
				if start_idx == -1:
					# this marks the start of a new segment
					start_idx = index
					prev_idx = index

				if (index - prev_idx > self.app_config.sequence_break_threshold):
					# there is a break in the sequence, so create an annotation entry
					annotation_list.append((frames[start_idx], frames[index]))
					# start a new segment
					start_idx = index

				prev_idx = index

			# progress eye candy
			if index % 300 == 0:
				print(".", end='', flush=True)
				if index % 300*50:
					print()

			index += 1

		# final segment
		if start_idx != -1:
			annotation_list.append((frames[start_idx], frames[index-1]))			
					
		print()					
		video_src.release()

		return annotation_list, boxes_list


	def writeAnnotationFile(self, filename, annotations):
		"""
		Writes a video annotation file, which lists the segments of video containing animals.

		NOTE TO MAINTAINERS: Be sure to keep the output format in sync with the "official"
		way to write Annotation objects, found in utils/annotation.py  The offical way is not
		used here because annotation.py imports PySide2 GUI modules, which is not copacetic
		with many of the GUI-less Docker images I use for machine learning.

		Inputs:
			filename		string; path to annotation file
			annotations		list of (startTime, endTime) segments		
		"""
		with open(filename, "w") as f:
			for start_time, end_time in annotations:
				f.write(f"1, AI_count, AI_count, {start_time}, {end_time}, AI\n")	


	def writeBoxFile(self, filename, boxes_list):
		"""
		Writes a video box file, which lists the bounding boxes containing animals.
		Inputs:
			filename		string; path to annotation file
			boxes_list		list of (index, list of bounding boxes) 		
		"""
		with open(filename, "w") as f:
			for index, boxes in boxes_list:
				for i in range(len(boxes)):
					yl, xl, yu, xu = boxes[i]
					f.write(f"{index},{yl},{xl},{yu},{xu},0,255,0\n")

					
def parseCommandLine():
	"""
	Parse the command line arguments.
	Returns a dictionary containing the arguments.
	"""
	ap = argparse.ArgumentParser()
	ap.add_argument("-c", "--config", required=False, default=None, help="path to configuration file")
	ap.add_argument("-i", "--input", required=True, help="path to input video")
	args = vars(ap.parse_args())
	return args


if __name__ == "__main__":
	args = parseCommandLine()
	app = ProcessVideo(args["config"])

	# assign any command line arguments to app configuration variables
	app.app_config.input_file = args["input"]

	app.main()


